{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collecting Tweets \n",
    "Using snc-scraper we were able to gather multiple ids for tweets that are from August 2017 to August 2020, however since these are only ids we are interested in the actual tweets and saving them in files to process and analyze. To do so we use the Twitter API to extract the tweets with the given ids and save them in a csv file to use later. \n",
    "The tweets are gathered in chunck of hundred as the Twitter API raises an error if their are more than 100 ids that it has to process at the same time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gatherIDs(fileName):\n",
    "    with open(fileName, \"r\") as f:\n",
    "        idList = [line.split(\"/\")[-1].strip() for line in f]\n",
    "        return idList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import pandas as pd\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tweepy, json\n",
    "\n",
    "consumer_key = \"\" # fill in with your API information \n",
    "consumer_secret = \"\"\n",
    "access_token = \"\"\n",
    "access_token_secret = \"\"\n",
    "\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_tweets_100(tweet_ids):     \n",
    "    \"\"\"\n",
    "    extracts 100 tweets with given tweet ids\n",
    "    \"\"\"\n",
    "    statuses = api.statuses_lookup(tweet_ids, tweet_mode=\"extended\")\n",
    "    data = pd.DataFrame(columns = [\"tweet_id\",\"name\",\"screen_name\",\"acctdesc\",\"text\", \"usercreated\", \"created_at\",\"favourite_count\",\"retweet_count\",\n",
    "                  \"hashtags\",\"status_count\", \"followers_count\",\"following_count\", \"location\", \"source_device\"]) # define your own dataframe\n",
    "    #print(data)\n",
    "    # printing the statuses\n",
    "    for status in statuses:\n",
    "        # print(status.lang)\n",
    "        \n",
    "        if status.lang == \"en\":\n",
    "            mined = {\n",
    "                \"tweet_id\": status.id,\n",
    "                \"name\": status.user.name,\n",
    "                \"screen_name\": status.user.screen_name,\n",
    "                \"acctdesc\" : status.user.description,\n",
    "                \"text\": status.full_text,\n",
    "                \"usercreated\": status.user.created_at,\n",
    "                \"created_at\": status.created_at,\n",
    "                \"favourite_count\": status.favorite_count,\n",
    "                \"retweet_count\": status.retweet_count,\n",
    "                \"hashtags\": status.entities[\"hashtags\"],\n",
    "                \"status_count\": status.user.statuses_count,\n",
    "                \"followers_count\": status.user.followers_count,\n",
    "                \"following_count\": status.user.friends_count,\n",
    "                \"location\": status.place,\n",
    "                \"source_device\": status.source\n",
    "            }\n",
    "\n",
    "            last_tweet_id = status.id\n",
    "            data = data.append(mined, ignore_index=True)\n",
    "            #print(data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def chuncks(id_list, n):\n",
    "    \"\"\"\n",
    "    helper function to break down the ids into chuncks of size n to avoid calling get_tweets_100 with too many ids\n",
    "    \"\"\"\n",
    "    # n is the number of elements in each list part\n",
    "    return [id_list[i * n:(i + 1) * n] for i in range((len(id_list) + n - 1) // n )]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_tweets(tweet_ids, tweetName):\n",
    "    dataOverall = pd.DataFrame(columns = [\"tweet_id\",\"name\",\"screen_name\",\"acctdesc\",\"text\", \"usercreated\", \"created_at\",\"favourite_count\",\"retweet_count\",\n",
    "                  \"hashtags\",\"status_count\", \"followers_count\", \"following_count\", \"location\", \"source_device\"])\n",
    "    numTot = len(tweet_ids)\n",
    "    if numTot > 100: \n",
    "        newList = chuncks(tweet_ids, 100)\n",
    "        for i in range(len(newList)):\n",
    "            data = get_tweets_100(newList[i])\n",
    "            #print(data)\n",
    "            dataOverall = dataOverall.append(data)\n",
    "            #print(dataOverall)\n",
    "    else:\n",
    "        dataOverall = get_tweets_100(tweet_ids)\n",
    "    \n",
    "    dataOverall.to_csv(\n",
    "        f\"tweets_\"+tweetName+\".csv\",header = [\"tweet_id\",\"name\",\"screen_name\",\"acctdesc\",\"text\",\"usercreated\",\n",
    "                                                        \"created_at\",\"favourite_count\", \"retweet_count\",\"hashtags\",\"status_count\", \n",
    "                                                        \"followers_count\", \"following_count\", \"location\", \"source_device\"], index=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = gatherIDs(\"Kill_Oct_tweets.txt\")\n",
    "len(ids)\n",
    "get_all_tweets(ids, \"autismtest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "killIds = gatherIDs(\"merged-kill.txt\")\n",
    "poisonIds = gatherIDs(\"merged-poison.txt\")\n",
    "autismIds = gatherIDs(\"merged-autism.txt\")\n",
    "injuredIds = gatherIDs(\"merged-injured.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "596"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(killIds)\n",
    "#len(injuredIds)\n",
    "len(autismIds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_all_tweets(killIds, \"vaccineskill\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "get_all_tweets(poisonIds, \"vaccinesarepoison\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_all_tweets(autismIds, \"vaccinescauseautism\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "get_all_tweets(injuredIds, \"vaccineinjured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "killIds2018 = gatherIDs(\"Kill_2018_tweets.txt\")\n",
    "poisonIds2018 = gatherIDs(\"Poison_2018_tweets.txt\")\n",
    "autismIds2018 = gatherIDs(\"Autism_2018_tweets.txt\")\n",
    "injuredIds2018 = gatherIDs(\"Injured_2018_all.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "get_all_tweets(killIds2018, \"vaccineskill2018\")\n",
    "get_all_tweets(poisonIds2018, \"poison2018\")\n",
    "get_all_tweets(autismIds2018, \"autism2018\")\n",
    "get_all_tweets(injuredIds2018, \"vaccineinjured2018\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "killIds2017 = gatherIDs(\"Kill_2017_tweets.txt\")\n",
    "poisonIds2017 = gatherIDs(\"Poison_2017_tweets.txt\")\n",
    "autismIds2017 = gatherIDs(\"Autism_2017_tweets.txt\")\n",
    "injuredIds2017 = gatherIDs(\"Injured2017.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "get_all_tweets(killIds2017, \"vaccineskill2017\")\n",
    "get_all_tweets(poisonIds2017, \"poison2017\")\n",
    "get_all_tweets(autismIds2017, \"autism2017\")\n",
    "get_all_tweets(injuredIds2017, \"vaccineinjured2017\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# getting #vaxxed tweets\n",
    "vaxxedIds2017 = gatherIDs(\"Vaxxed2017.txt\")\n",
    "get_all_tweets(vaxxedIds2017, \"vaxxed2017\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "vaxxedIds2018 = gatherIDs(\"Vaxxed2018.txt\")\n",
    "get_all_tweets(vaxxedIds2018, \"vaxxed2018\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vaxxedIds = gatherIDs(\"Vaxxed2019.txt\")\n",
    "get_all_tweets(vaxxedIds, \"vaxxed2019\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
