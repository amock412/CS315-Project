{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PM2 - Data Collection "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My initial plan was to collect Twitter data using the Python library GetOldTweets3. GetOldTweets3 is a library that allows you to scrape old tweets and does not require you to use the official Twitter API. However as of Thursday, September 17, 2020, Twitter no longer allows individuals to access tweets via GetOldTweets3 (for more information about this issue: https://github.com/Mottl/GetOldTweets3/issues/98 \n",
    "\n",
    "For reasons of completeness I have included some of the code that I wrote to use GetOldTweets3 for older Twitter data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 - Collecting data via GetOldTweets3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import GetOldTweets3 as got"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My collection strategy was to collect Tweets via GetOldTweets3 in batches of one month from May 2019 to May 2020 to explore the changes in anti-vaccine sentiment on Twitter. The following is an example query I used to get the Tweets in the month of May 2019. Due to the fact that GetOldTweets3 no longer is working I was unable to collect and save any data from my old queries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_query = '#vaccinesarepoison'\n",
    "since_date = '2020-05-01'\n",
    "until_date = '2020-05-07'\n",
    "count = 1000\n",
    "# Creation of query object\n",
    "# tweetCriteria = got.manager.TweetCriteria().setQuerySearch(text_query).setSince(since_date).setUntil(until_date).setMaxTweets(count)\n",
    "tweetCriteria = got.manager.TweetCriteria().setQuerySearch(text_query).setSince(since_date).setUntil(until_date)\n",
    "\n",
    "# Creation of list that contains all tweets\n",
    "tweets = got.manager.TweetManager.getTweets(tweetCriteria)\n",
    "\n",
    "# Creating list of chosen tweet data\n",
    "text_tweets = [[tweet.id, tweet.date, tweet.text, tweet.username, tweet.favorites] for tweet in tweets]\n",
    "\n",
    "# Creation of dataframe from tweets list\n",
    "tweets_df = pd.DataFrame(text_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From my initial search I realized that particular accounts where posting on the topic more than others. In order to get a better sense of what these accounts were posting in general I decided to look into them individually (see screenshots of code in data exploration notebook). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 - Collecting data via Tweepy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After GetOldTweets3 stopped working I decided to shift my focus to Tweepy another Python library that allows one to extract tweets directly with the help of Twitter's API. We first begin by instantiating a search opject using our consumer key and token provided via the Twitter developer account. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Authentication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tweepy as tw\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "consumer_key = '' # actual values omitted for security reasons\n",
    "consumer_secret = ''\n",
    "access_token = ''\n",
    "access_token_secret = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "auth = tw.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tw.API(auth, wait_on_rate_limit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tweepy.api.API at 0x7fbda6722ad0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "api"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Perform search and store data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After authentication we can perform our searches. Here I began by searching for the terms vaccines are poison. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "search_term = \"vaccines+are+poison-filter:retweets\" # search term without retweets\n",
    "\n",
    "tweets = tw.Cursor(api.search,\n",
    "                   q=search_term,\n",
    "                   lang=\"en\",\n",
    "                   since='2019-09-01').items(1000)\n",
    "\n",
    "all_tweets = [tweet.text for tweet in tweets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"@Frank38328140 @marklevinshow There's a reason why Bill Gate's is banned from entering certain countries. His vacci… https://t.co/sYXwN55RSs\",\n",
       " 'Vaccines are poison. &amp;  bio weapons. \\nNever take them. https://t.co/NonXieJmKu',\n",
       " 'Just step away from the vaccines and viruses Gates. Viruses in humans &amp; computers are not the same thing so quit tr… https://t.co/eZ4jYG1cXW']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_tweets[5:8] # show a small subset of the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save the data in a text file for data exploration\n",
    "fName = \"tweets_1.txt\"\n",
    "with open(fName, 'w') as f:\n",
    "    for tweet in all_tweets:\n",
    "         f.write(tweet + '\\n') \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3 - More sophisticated search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After our basic search performed above, I decided that it would be good to not only analyze sentiment but also gather \n",
    "broader data that would allow me to do more with the data, similar to my original plan of using GetOldTweets to analyze anti-vaccination tweets over the course of one year. In the course of searching for a more sophisticated method to extract tweets I came accross some code by the user Leow Griffin on Github (https://github.com/leowgriffin/tweets_analysis_hkprotests_2019/blob/master/scraping_tweets.py). I have adapted some of his code for my purposes of creating a csv that contains relevant tweets for my project. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import csv\n",
    "import re\n",
    "from textblob import TextBlob\n",
    "import string\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getTweets(search_words, date_since, numTweets, numRuns):\n",
    "    \"\"\"\n",
    "    Gets the tweets that are contained in the search query (search_words) starting at date_since, \n",
    "    with a maximum of numTweets and performs the API call numRuns times\n",
    "    Data is saved in a CSV file \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Define a pandas dataframe to store the date:\n",
    "    db_tweets = pd.DataFrame(columns = ['username', 'acctdesc', 'location', 'following',\n",
    "                                        'followers', 'totaltweets', 'usercreatedts', 'tweetcreatedts',\n",
    "                                        'retweetcount', 'text', 'hashtags']\n",
    "                                )\n",
    "    \n",
    "    # Define a for-loop to generate tweets at regular intervals\n",
    "    for i in range(0, numRuns):\n",
    "        \n",
    "        # Collect tweets \n",
    "        tweets = tw.Cursor(api.search, q=search_words, lang=\"en\", since=date_since, tweet_mode='extended').items(numTweets)\n",
    "\n",
    "        # Store these tweets into a list\n",
    "        tweet_list = [tweet for tweet in tweets]\n",
    "\n",
    "        # Obtain the following info (methods to call them out):\n",
    "            # user.screen_name - twitter handle\n",
    "            # user.description - description of account\n",
    "            # user.location - where is he tweeting from\n",
    "            # user.friends_count - no. of other users that user is following (following)\n",
    "            # user.followers_count - no. of other users who are following this user (followers)\n",
    "            # user.statuses_count - total tweets by user\n",
    "            # user.created_at - when the user account was created\n",
    "            # created_at - when the tweet was created\n",
    "            # retweet_count - no. of retweets\n",
    "            # (deprecated) user.favourites_count - probably total no. of tweets that is favourited by user\n",
    "            # retweeted_status.full_text - full text of the tweet\n",
    "            # tweet.entities['hashtags'] - hashtags in the tweet\n",
    "\n",
    "        # Begin scraping the tweets individually:\n",
    "        noTweets = 0\n",
    "\n",
    "        for tweet in tweet_list:\n",
    "\n",
    "            # Pull the values\n",
    "            username = tweet.user.screen_name\n",
    "            acctdesc = tweet.user.description\n",
    "            location = tweet.user.location\n",
    "            following = tweet.user.friends_count\n",
    "            followers = tweet.user.followers_count\n",
    "            totaltweets = tweet.user.statuses_count\n",
    "            usercreatedts = tweet.user.created_at\n",
    "            tweetcreatedts = tweet.created_at\n",
    "            retweetcount = tweet.retweet_count\n",
    "            hashtags = tweet.entities['hashtags']\n",
    "\n",
    "            try:\n",
    "                text = tweet.retweeted_status.full_text\n",
    "            except AttributeError:  # Not a Retweet\n",
    "                text = tweet.full_text\n",
    "\n",
    "            # Add the 11 variables to the empty list - ith_tweet:\n",
    "            ith_tweet = [username, acctdesc, location, following, followers, totaltweets,\n",
    "                         usercreatedts, tweetcreatedts, retweetcount, text, hashtags]\n",
    "\n",
    "            # Append to dataframe - db_tweets\n",
    "            db_tweets.loc[len(db_tweets)] = ith_tweet\n",
    "\n",
    "            # increase counter - noTweets  \n",
    "            noTweets += 1\n",
    "\n",
    "        \n",
    "        print('no. of tweets scraped for run {} is {}'.format(i, noTweets))\n",
    "        \n",
    "        time.sleep(300) #5 minute sleep time\n",
    "\n",
    "\n",
    "    # Define working path and filename\n",
    "    path = os.getcwd()\n",
    "    filename = path + '/data/' + search_words + '_twitter_data.csv'\n",
    "\n",
    "    # Store dataframe in csv with creation date timestamp\n",
    "    db_tweets.to_csv(filename, index = False)\n",
    "    \n",
    "    print('Scraping has completed!') # notify when done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "query1 = 'vaccines+are+poison' # testing query to see if our getTweets function works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no. of tweets scraped for run 0 is 1000\n",
      "no. of tweets scraped for run 1 is 1000\n",
      "Scraping has completed!\n"
     ]
    }
   ],
   "source": [
    "getTweets(query1, '2020-09-03', 1000, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "queries = ['vaccines+are+poison', 'vaccines+kill','vaccines+cause+aids', 'vaccines+cause+autism' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no. of tweets scraped for run 0 is 2597\n",
      "no. of tweets scraped for run 1 is 2597\n",
      "Scraping has completed!\n"
     ]
    }
   ],
   "source": [
    "for query in queries:\n",
    "    getTweets(query, '2020-09-03', 3000, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
